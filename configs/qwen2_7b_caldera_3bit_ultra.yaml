# 3-bit with ultra-aggressive fidelity allocation for Qwen2-7B
# Based on layer_fidelity.json: late layers are EXTREMELY sensitive
# Layer 27: 0.006, Layer 26: 0.016, Layer 25: 0.026, Layer 24: 0.044
# Early layers (0-2) are robust (0.88-0.98)
# Budget: 2*256 + 2*128 + 1*80 + 23*32 = 1584 (uniform = 1792)

model_id: Qwen/Qwen2-7B-Instruct
output_dir: artifacts/qwen2-7b/caldera-3bit-ultra

calibration:
  datasets:
    - name: c4
      weight: 1.0
  samples: 512
  sequence_length: 512
  progress_every: 100

caldera:
  bq: 3
  bl: 4
  br: 4
  rank: 32   # Default - minimal for robust early layers
  group_size: 128
  calibration_batch_size: 1
  calibration_samples: 512
  use_calibration: false
  ridge_lambda: 1.0
  compute_device: cuda
  target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj

  pattern_overrides:
    # CRITICAL: Final layers are near-random without protection
    "model.layers.27.*": {rank: 256}  # fidelity 0.006 (near-random!)
    "model.layers.26.*": {rank: 256}  # fidelity 0.016

    # VERY HIGH sensitivity
    "model.layers.25.*": {rank: 128}  # fidelity 0.026
    "model.layers.24.*": {rank: 128}  # fidelity 0.044

    # ELEVATED sensitivity
    "model.layers.23.*": {rank: 80}   # fidelity 0.130

  use_rht: false
  lora_finetune: false

runtime:
  dtype: bf16
  device: cuda
