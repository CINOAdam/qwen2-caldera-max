# 3-bit with ultra-aggressive fidelity allocation for Qwen2-72B
# Based on 7B pattern: late layers are EXTREMELY sensitive
# 80 layers total - extrapolating sensitivity pattern
# Budget: 5*256 + 5*128 + 5*80 + 65*32 = 4400 (uniform = 5120, -14%)

model_id: Qwen/Qwen2-72B-Instruct
output_dir: artifacts/qwen2-72b/caldera-3bit-ultra

calibration:
  datasets:
    - name: c4
      weight: 1.0
  samples: 512
  sequence_length: 512
  progress_every: 100

caldera:
  bq: 3
  bl: 4
  br: 4
  rank: 32   # Default - minimal for robust layers
  group_size: 128
  calibration_batch_size: 1
  calibration_samples: 512
  use_calibration: false
  ridge_lambda: 1.0
  compute_device: cuda
  target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj

  pattern_overrides:
    # CRITICAL: Last 5 layers (75-79) - expect near-random without protection
    "model.layers.79.*": {rank: 256}
    "model.layers.78.*": {rank: 256}
    "model.layers.77.*": {rank: 256}
    "model.layers.76.*": {rank: 256}
    "model.layers.75.*": {rank: 256}

    # VERY HIGH sensitivity (70-74)
    "model.layers.74.*": {rank: 128}
    "model.layers.73.*": {rank: 128}
    "model.layers.72.*": {rank: 128}
    "model.layers.71.*": {rank: 128}
    "model.layers.70.*": {rank: 128}

    # ELEVATED sensitivity (65-69)
    "model.layers.69.*": {rank: 80}
    "model.layers.68.*": {rank: 80}
    "model.layers.67.*": {rank: 80}
    "model.layers.66.*": {rank: 80}
    "model.layers.65.*": {rank: 80}

  use_rht: false
  lora_finetune: false

runtime:
  dtype: bf16
  device: cuda
